## **A) Effectiveness vs. Efficiency**

- **Effectiveness** is about how well a system helps users achieve their goals **accurately and completely**. In other words, it’s about **success** — can the user actually do what they set out to do?

- **Efficiency**, on the other hand, looks at the **resources needed** (like time or effort) to reach those goals. It’s about **speed and ease** — how quickly or smoothly can the user get it done?


**Example:**

- A **physical keyboard** tends to be **more effective**, since the tactile feedback allows for more accurate typing and fewer mistakes.
    
- A **touchscreen keyboard**, however, might be **more efficient** in certain situations (like sending quick texts on a phone) because it’s faster to access and can use predictive text.
    

---

## **B) System Usability Scale (SUS)**

**What does SUS measure?**  
The SUS measures the **overall perceived usability** of a system. It reflects a user’s general impression of how usable, easy, and satisfying an interface feels — not specific performance stats, but the overall experience.

**What’s a good SUS score?**  
SUS scores range from **0 to 100**:

- Around **68** = average usability
    
- **Above 80** = excellent usability (Grade A)
    
- **Below 50** = poor usability
    

**Why use post-task questionnaires as well?**  
While SUS gives you a big-picture view of usability after the whole test, **post-task questionnaires** capture how users felt **after each specific task**. This helps pinpoint where problems occur — the SUS tells you _that_ there’s a usability issue, and post-task feedback helps show _where_ and _why_.

---

## **C) Experimental Design Table**

|Question|Factor|Levels|Measurement|Design|
|---|---|---|---|---|
|**Q1:** Do users prefer pie menus over square menus?|Menu type|Pie menu, Square menu|Preference rating or number of selections|**Within-subjects** (each user tries both menus)|
|**Q2:** Which booking app do middle-aged men find easiest for restaurant reservations?|App used|EatOut, YumYum, DinDin|Task completion time, number of errors, or success rate|**Within-subjects** (each user tests all apps)|
|**Q3:** Does red-green color deficiency affect reaction time to traffic lights?|Color vision|Normal, Red-green deficiency|Reaction time (in seconds)|**Between-subjects** (different participant groups)|

---

## **D) Levels of Measurement (Data Types)**

|Variable|Type|Level of Measurement|
|---|---|---|
|Menu type|Factor|Nominal|
|Preference rating|Measurement|Ordinal (e.g., Likert scale)|
|App used|Factor|Nominal|
|Completion time|Measurement|Ratio|
|Errors/success rate|Measurement|Ratio|
|Colour vision|Factor|Nominal|
|Reaction time|Measurement|Ratio|

|Level|Description|Example|What You Can Do With It|
|---|---|---|---|
|**Nominal**|Categories with no numerical meaning. Just shows if things are the same or different.|Gender, app name, menu type|Count, mode, percentages|
|**Ordinal**|Ordered categories — ranked, but not evenly spaced.|Likert scales, satisfaction ratings|Median, rank correlation|
|**Interval**|Numeric values with equal spacing but no true zero.|Temperature (°C), time of day|Mean, SD, t-tests|
|**Ratio**|Like interval but with a true zero — so ratios make sense.|Reaction time, number of errors, completion time|All statistical operations, including ratios|

---

## **E) Counterbalancing**

Counterbalancing is used when participants do **multiple conditions** in a study. It helps control for **order effects**, like learning or fatigue, which might affect performance.

- **Q1:** Yes — it’s a within-subjects design, so counterbalance which menu type participants try first.
    
- **Q2:** Yes — also within-subjects, so balance the order in which each app is tested.
    
- **Q3:** No — between-subjects design, so each person only does one condition, and counterbalancing isn’t needed.