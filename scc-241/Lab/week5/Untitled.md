![[Pasted image 20251106200524.png]]**A) Effectiveness vs. Efficiency**

- **Effectiveness** means how well a system enables users to achieve their goals accurately and completely. It’s about _success_ — can the user do what they intended?
    
- **Efficiency** refers to the resources (time, effort) required to achieve those goals. It’s about _speed and effort_ — how quickly or easily can the user do it?
    

**Example:**

- A **physical keyboard** is often **more effective** because users can type accurately with tactile feedback, leading to fewer typing errors.
    
- A **touchscreen keyboard** may be **more efficient** in some cases (e.g., quick short messages on mobile devices) since it allows for predictive text and requires less setup or space.
    

---

### **B) System Usability Scale (SUS)**

**• What does SUS measure?**  
It measures **overall perceived usability** of a system — a user’s subjective assessment of how usable, easy, and satisfactory the interface is. It captures general usability impressions rather than specific performance metrics.

**• What is a good score?**  
SUS scores range from 0 to 100.

- Average usability ≈ **68**
    
- Good usability: **> 80 (Grade A)**
    
- Poor usability: **< 50**
    

**• Why use post-task questionnaires too?**  
Post-task questionnaires provide **immediate feedback on specific tasks**, identifying where users struggled. The SUS (a post-test questionnaire) gives **overall usability impressions**, while post-task questionnaires capture **task-level experiences**. Using both helps identify _when_ and _why_ problems occur.

---

### **C) Experimental Design Table**

|Question|Factor|Levels|Measurement|Design|
|---|---|---|---|---|
|**Q1**: Do users prefer pie menus over square menus?|Menu type|Pie menu, Square menu|Preference rating or number of selections favoring one type|**Within-subjects** (each user tries both menus)|
|**Q2**: On which booking app do middle-aged men find it easiest to complete a restaurant booking?|App used|EatOut, YumYum, DinDin|Task completion time, number of errors, or success rate|**Within-subjects** (each user tests all apps)|
|**Q3**: Does red-green colour deficiency affect reaction time to traffic lights?|Colour vision|Normal, Red-green deficiency|Reaction time (in seconds)|**Between-subjects** (participants belong to different vision groups)|

---

### **D) Levels of Measurement (Data Types)**

|Variable|Type|Level of Measurement|
|---|---|---|
|**Menu type**|Factor|Nominal|
|**Preference rating**|Measurement|Ordinal (e.g., Likert scale)|
|**App used**|Factor|Nominal|
|**Completion time**|Measurement|Ratio|
|**Errors/success rate**|Measurement|Ratio (count, percentage)|
|**Colour vision**|Factor|Nominal|
|**Reaction time**|Measurement|Ratio|

---

### **E) Counterbalancing**

Counterbalancing is needed **whenever the same participants perform multiple conditions**, to control for **order effects** (learning or fatigue).

- **Q1:** Yes — within-subjects, so counterbalance which menu type is tested first.
    
- **Q2:** Yes — within-subjects, so counterbalance the order of apps tested.
    
- **Q3:** No — between-subjects design, no counterbalancing needed since each participant only experiences one condition.