![[Pasted image 20251102031223.png]]![[Pasted image 20251102031235.png]]![[Pasted image 20251102031243.png]]1. Why do threads help here? Explain how blocking I/O allows multiple Python threads to make progress.
    
    > When a thread performs blocking I/O (e.g., waiting for data from a network or disk), it releases the Global Interpreter Lock (GIL), allowing other threads to run. This means that while one thread is waiting for I/O, other threads can execute their tasks, effectively overlapping I/O wait times with computation. This concurrency allows better utilization of CPU resources, as threads can continue processing while others are blocked on I/O operations. The resulting behavior matches what the select blocking call achieves, but with a simpler programming model using threads.
    
2. How does this shared-memory approach differ from Task 1A and 1B (pipes and select)? What are the trade-offs?
    
    > In Task 1A and 1B, inter-process communication (IPC) is achieved using pipes and the select system call, which involves separate processes with their own memory spaces. This approach provides strong isolation between processes, enhancing security and stability, but comes with higher overhead due to context switching and data copying between processes. In contrast, Task 2A uses threads within a single process that share the same memory space. This shared-memory approach allows for faster communication and lower overhead since threads can directly access shared data structures without the need for serialization or IPC mechanisms. Furthermore, the fact that Python is not a multi-processor language due to the GIL means that the benefits of threads are limited, since threads cannot use multiple CPU cores in parallel for CPU-bound tasks.
    
3. Scaling limits. Why do speedups diminish as you add more threads?
    
    > After a certain number of threads, the speed up of parallelism has a lower effect. As long as there enough threads to execute processing while other threads are blocking, is sufficient in order to use the available CPU efficiently. Adding more threads beyond this point has little effect on performance and it can even affect negatively due to increased overhead from thread management, and contention for shared resources.